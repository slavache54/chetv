import asyncio
import aiohttp
import re
import os
import sys
from collections import defaultdict

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
SOURCES_FILE = 'sources.txt'
OUTPUT_FILE = 'master_playlist.m3u'
# DEFAULT_CATEGORY –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ –ø—É—Å—Ç—å –æ—Å—Ç–∞–µ—Ç—Å—è
DEFAULT_CATEGORY = '–û–±—â–∏–µ' 
MAX_CONCURRENT_REQUESTS = 200
TIMEOUT = 5
CHUNK_SIZE = 2048

BAD_CONTENT_TYPES = ['text/html', 'application/json', 'image/']
GOOD_CONTENT_TYPES = ['video/', 'application/vnd.apple.mpegurl', 'application/x-mpegurl']
HEADERS = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36' }

def load_sources():
    sources = []
    if not os.path.exists(SOURCES_FILE):
        return sources
    with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            parts = line.split(',', 1)
            if len(parts) == 2:
                name, url = parts[0].strip(), parts[1].strip()
                sources.append({'name': name, 'url': url})
            else:
                name = f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i + 1}"
                url = line
                sources.append({'name': name, 'url': url})
    return sources

def parse_m3u_content(content):
    channels = []
    pattern = re.compile(r'#EXTINF:-1.*?,([^\n]*)\n(https?://[^\n]*)')
    matches = pattern.findall(content)
    for name, url in matches:
        clean_name = name.strip()
        if clean_name and url:
            # –ú—ã –±–æ–ª—å—à–µ –Ω–µ –ø–∞—Ä—Å–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–¥–µ—Å—å, –æ–Ω–∞ –±—É–¥–µ—Ç –ø—Ä–∏—Å–≤–æ–µ–Ω–∞ –ø–æ–∑–∂–µ
            channels.append({'name': clean_name, 'url': url.strip()})
    return channels

async def check_stream_url(session, channel, semaphore):
    async with semaphore:
        try:
            async with session.get(channel['url'], timeout=TIMEOUT, allow_redirects=True) as response:
                if not (200 <= response.status < 400): return None
                content_type = response.headers.get('Content-Type', '').lower()
                if any(bad_type in content_type for bad_type in BAD_CONTENT_TYPES): return None
                is_good_type = any(good_type in content_type for good_type in GOOD_CONTENT_TYPES)
                try:
                    chunk = await response.content.read(CHUNK_SIZE)
                except (aiohttp.ClientError, asyncio.TimeoutError):
                    return None
                if not chunk: return None
                if chunk.count(b'\x47') > 5: return channel
                if is_good_type: return channel
                return None
        except (aiohttp.ClientError, asyncio.TimeoutError, ConnectionResetError):
            return None
        return None

async def main():
    print("--- –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ---")
    sources = load_sources()
    if not sources:
        print(f"[–û–®–ò–ë–ö–ê] –§–∞–π–ª '{SOURCES_FILE}' –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç.")
        return
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(sources)} –ø–ª–µ–π–ª–∏—Å—Ç–æ–≤-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")
    final_header = '#EXTM3U'
    epg_found = False
    all_channels = []
    async with aiohttp.ClientSession(headers=HEADERS) as session:
        for source in sources:
            source_name = source['name']
            url = source['url']
            try:
                print(f"  –ó–∞–≥—Ä—É–∑–∫–∞: {source_name} ({url})")
                async with session.get(url, timeout=15) as response:
                    response.raise_for_status()
                    content = await response.text()
                    if not epg_found:
                        for line in content.splitlines():
                            if line.strip().startswith("#EXTM3U") and 'url-tvg' in line:
                                final_header = line.strip(); epg_found = True; print(f"    -> –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å EPG."); break
                    
                    parsed_channels = parse_m3u_content(content)
                    
                    # --- –í–û–¢ –ì–õ–ê–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï ---
                    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –í–°–ï–ú –∫–∞–Ω–∞–ª–∞–º –∏–∑ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é, —Ä–∞–≤–Ω—É—é –∏–º–µ–Ω–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                    for ch in parsed_channels:
                        ch['category'] = source_name
                    
                    all_channels.extend(parsed_channels)
                    print(f"    -> –ù–∞–π–¥–µ–Ω–æ {len(parsed_channels)} –∫–∞–Ω–∞–ª–æ–≤.")
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                print(f"    -> –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {e}")

    if not all_channels: print("\n–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏."); return
    print(f"\n–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_channels)} –∫–∞–Ω–∞–ª–æ–≤. –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è '–£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞'...")
    categorized_working_channels = defaultdict(list)
    unique_urls = set()
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    async with aiohttp.ClientSession(headers=HEADERS) as session:
        tasks = [check_stream_url(session, channel, semaphore) for channel in all_channels]
        total = len(tasks)
        for i, future in enumerate(asyncio.as_completed(tasks), 1):
            result = await future
            sys.stdout.write(f"\r–ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total} ({i/total*100:.1f}%)"); sys.stdout.flush()
            if result and result['url'] not in unique_urls:
                unique_urls.add(result['url']); categorized_working_channels[result['category']].append(result)

    print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∏–º–µ–Ω–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    sorted_categories = sorted(categorized_working_channels.keys())
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(f"{final_header}\n")
        for category in sorted_categories:
            channels_in_category = sorted(categorized_working_channels[category], key=lambda x: x['name'])
            for channel in channels_in_category:
                f.write(f'#EXTINF:-1 group-title="{channel["category"]}",{channel["name"]}\n')
                f.write(f'{channel["url"]}\n')
    total_working = len(unique_urls)
    print("\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ---")
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π –ø–ª–µ–π–ª–∏—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {OUTPUT_FILE}")
    print(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏ —Ä–∞–±–æ—á–∏—Ö –∫–∞–Ω–∞–ª–æ–≤: {total_working}")
    print(f"üóëÔ∏è  –û—Ç—Å–µ—è–Ω–æ –Ω–µ—Ä–∞–±–æ—á–∏—Ö –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(all_channels) - total_working}")

if __name__ == '__main__':
    asyncio.run(main())
