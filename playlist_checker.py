import asyncio
import aiohttp
import re
import os
import sys
from collections import defaultdict

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
SOURCES_FILE = 'sources.txt'
OUTPUT_FILE = 'master_playlist.m3u'
DEFAULT_CATEGORY = '–û–±—â–∏–µ'
MAX_CONCURRENT_REQUESTS = 200  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
TIMEOUT = 4  # –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def parse_m3u_content(content):
    """–ü–∞—Ä—Å–∏—Ç M3U –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞–Ω–∞–ª—ã —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏."""
    channels = []
    pattern = re.compile(r'#EXTINF:-1(.*?),([^\n]*)\n(https?://[^\n]*)')
    matches = pattern.findall(content)

    for attributes, name, url in matches:
        group_title_match = re.search(r'group-title="(.*?)"', attributes, re.IGNORECASE)
        category = group_title_match.group(1).strip() if group_title_match else DEFAULT_CATEGORY
        clean_name = name.strip()

        if clean_name and url:
            channels.append({'name': clean_name, 'url': url.strip(), 'category': category})
    return channels

async def check_stream_url(session, channel, semaphore):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å URL."""
    async with semaphore:
        try:
            async with session.head(channel['url'], timeout=TIMEOUT, allow_redirects=True) as response:
                if 200 <= response.status < 400:
                    return channel
        except (aiohttp.ClientError, asyncio.TimeoutError):
            pass
        return None

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    print("--- –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–ª–µ–π–ª–∏—Å—Ç–æ–≤ ---")

    if not os.path.exists(SOURCES_FILE):
        print(f"[–û–®–ò–ë–ö–ê] –§–∞–π–ª '{SOURCES_FILE}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
        source_urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    
    if not source_urls:
        print("[–ò–ù–§–û] –§–∞–π–ª –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø—É—Å—Ç.")
        return
        
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(source_urls)} –ø–ª–µ–π–ª–∏—Å—Ç–æ–≤-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")

    final_header = '#EXTM3U'
    epg_found = False
    all_channels = []

    async with aiohttp.ClientSession(headers=HEADERS) as session:
        for url in source_urls:
            try:
                print(f"  –ó–∞–≥—Ä—É–∑–∫–∞: {url}")
                async with session.get(url, timeout=15) as response:
                    response.raise_for_status()
                    content = await response.text()

                    if not epg_found:
                        for line in content.splitlines():
                            if line.strip().startswith("#EXTM3U") and 'url-tvg' in line:
                                final_header = line.strip()
                                epg_found = True
                                print(f"    -> –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å EPG.")
                                break
                    
                    channels = parse_m3u_content(content)
                    all_channels.extend(channels)
                    print(f"    -> –ù–∞–π–¥–µ–Ω–æ {len(channels)} –∫–∞–Ω–∞–ª–æ–≤.")
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                print(f"    -> –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {e}")

    if not all_channels:
        print("\n–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.")
        return

    print(f"\n–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_channels)} –∫–∞–Ω–∞–ª–æ–≤. –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞...")
    
    categorized_working_channels = defaultdict(list)
    unique_urls = set()
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

    async with aiohttp.ClientSession(headers=HEADERS) as session:
        tasks = [check_stream_url(session, channel, semaphore) for channel in all_channels]
        total = len(tasks)
        for i, future in enumerate(asyncio.as_completed(tasks), 1):
            result = await future
            sys.stdout.write(f"\r–ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total} ({i/total*100:.1f}%)")
            sys.stdout.flush()

            if result and result['url'] not in unique_urls:
                unique_urls.add(result['url'])
                categorized_working_channels[result['category']].append(result)

    print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    
    sorted_categories = sorted(categorized_working_channels.keys())
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(f"{final_header}\n")
        for category in sorted_categories:
            channels_in_category = sorted(categorized_working_channels[category], key=lambda x: x['name'])
            for channel in channels_in_category:
                f.write(f'#EXTINF:-1 group-title="{channel["category"]}",{channel["name"]}\n')
                f.write(f'{channel["url"]}\n')

    total_working = len(unique_urls)
    print("\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ---")
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π –ø–ª–µ–π–ª–∏—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {OUTPUT_FILE}")
    print(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏ —Ä–∞–±–æ—á–∏—Ö –∫–∞–Ω–∞–ª–æ–≤: {total_working}")
    print(f"üóëÔ∏è  –û—Ç—Å–µ—è–Ω–æ –Ω–µ—Ä–∞–±–æ—á–∏—Ö –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(all_channels) - total_working}")

if __name__ == '__main__':
    asyncio.run(main())