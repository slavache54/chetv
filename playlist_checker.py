import asyncio
import aiohttp
import re
import os
import sys
from collections import defaultdict

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
SOURCES_FILE = 'sources.txt'
OUTPUT_FILE = 'master_playlist.m3u'
DEFAULT_CATEGORY = '–û–±—â–∏–µ'
MAX_CONCURRENT_REQUESTS = 200
TIMEOUT = 5
CHUNK_SIZE = 2048

# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: "–ú–∞—Å–∫–∏—Ä—É–µ–º—Å—è" –ø–æ–¥ VLC –ø–ª–µ–µ—Ä ---
HEADERS = {
    'User-Agent': 'VLC/3.0.18 LibVLC/3.0.18',
    'Accept': '*/*'
}

BAD_CONTENT_TYPES = ['text/html', 'application/json', 'image/']
GOOD_CONTENT_TYPES = ['video/', 'application/vnd.apple.mpegurl', 'application/x-mpegurl', 'application/octet-stream']

def load_sources():
    # ... (—ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
    sources = []
    if not os.path.exists(SOURCES_FILE): return sources
    with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            line = line.strip()
            if not line or line.startswith('#'): continue
            parts = line.split(',', 1)
            if len(parts) == 2:
                name, url = parts[0].strip(), parts[1].strip()
                sources.append({'name': name, 'url': url})
            else:
                name = f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i + 1}"; url = line
                sources.append({'name': name, 'url': url})
    return sources

def parse_m3u_content(content):
    # ... (—ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
    channels = []
    pattern = re.compile(r'#EXTINF:-1.*?,([^\n]*)\n(https?://[^\n]*)')
    matches = pattern.findall(content)
    for name, url in matches:
        clean_name = name.strip()
        if clean_name and url:
            channels.append({'name': clean_name, 'url': url.strip()})
    return channels

async def check_stream_url(session, channel, semaphore):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è "–£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞" - —Å–º—è–≥—á–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è."""
    async with semaphore:
        try:
            async with session.get(channel['url'], timeout=TIMEOUT, allow_redirects=True) as response:
                if not (200 <= response.status < 400): return None

                content_type = response.headers.get('Content-Type', '').lower()
                if any(bad_type in content_type for bad_type in BAD_CONTENT_TYPES):
                    return None
                
                # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–º—è–≥—á–∞–µ–º –ª–æ–≥–∏–∫—É ---
                # –ï—Å–ª–∏ Content-Type —è–≤–Ω–æ —Ö–æ—Ä–æ—à–∏–π (–¥–∞–∂–µ application/octet-stream), —Å—á–∏—Ç–∞–µ–º –∫–∞–Ω–∞–ª —Ä–∞–±–æ—á–∏–º
                if any(good_type in content_type for good_type in GOOD_CONTENT_TYPES):
                    return channel

                # –ï—Å–ª–∏ Content-Type –Ω–µ—è—Å–µ–Ω, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–≥–Ω–∞—Ç—É—Ä—É –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–∞–¥–µ–∂–¥—É
                try:
                    chunk = await response.content.read(CHUNK_SIZE)
                    if chunk and chunk.count(b'\x47') > 5:
                        return channel
                except (aiohttp.ClientError, asyncio.TimeoutError):
                    return None
                
                # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–æ –∏–∑ —É—Å–ª–æ–≤–∏–π –Ω–µ –≤—ã–ø–æ–ª–Ω–∏–ª–æ—Å—å, –æ—Ç–±—Ä–∞–∫–æ–≤—ã–≤–∞–µ–º
                return None

        except (aiohttp.ClientError, asyncio.TimeoutError, ConnectionResetError):
            return None
        return None

async def main():
    # ... (–≤—Å—è –æ—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å main –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –∫—Ä–æ–º–µ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è) ...
    print("--- –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ —Å –æ–±—Ö–æ–¥–æ–º –∑–∞—â–∏—Ç—ã –∏ —Å–º—è–≥—á–µ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π ---")
    sources = load_sources()
    if not sources:
        print(f"[–û–®–ò–ë–ö–ê] –§–∞–π–ª '{SOURCES_FILE}' –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç."); return
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(sources)} –ø–ª–µ–π–ª–∏—Å—Ç–æ–≤-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")
    final_header = '#EXTM3U'; epg_found = False; all_channels = []
    async with aiohttp.ClientSession(headers=HEADERS) as session:
        for source in sources:
            source_name = source['name']; url = source['url']
            try:
                print(f"  –ó–∞–≥—Ä—É–∑–∫–∞: {source_name} ({url})")
                async with session.get(url, timeout=15) as response:
                    response.raise_for_status(); content = await response.text()
                    if not epg_found:
                        for line in content.splitlines():
                            if line.strip().startswith("#EXTM3U") and 'url-tvg' in line:
                                final_header = line.strip(); epg_found = True; print(f"    -> –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å EPG."); break
                    parsed_channels = parse_m3u_content(content)
                    for ch in parsed_channels:
                        ch['category'] = source_name
                    all_channels.extend(parsed_channels)
                    print(f"    -> –ù–∞–π–¥–µ–Ω–æ {len(parsed_channels)} –∫–∞–Ω–∞–ª–æ–≤.")
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                print(f"    -> –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {e}")

    if not all_channels: print("\n–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏."); return
    print(f"\n–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_channels)} –∫–∞–Ω–∞–ª–æ–≤. –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è '–£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞'...")
    categorized_working_channels = defaultdict(list)
    unique_urls = set()
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    async with aiohttp.ClientSession(headers=HEADERS) as session:
        tasks = [check_stream_url(session, channel, semaphore) for channel in all_channels]
        total = len(tasks)
        for i, future in enumerate(asyncio.as_completed(tasks), 1):
            result = await future
            sys.stdout.write(f"\r–ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total} ({i/total*100:.1f}%)"); sys.stdout.flush()
            if result and result['url'] not in unique_urls:
                unique_urls.add(result['url']); categorized_working_channels[result['category']].append(result)

    print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    sorted_categories = sorted(categorized_working_channels.keys())
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(f"{final_header}\n")
        for category in sorted_categories:
            channels_in_category = sorted(categorized_working_channels[category], key=lambda x: x['name'])
            for channel in channels_in_category:
                f.write(f'#EXTINF:-1 group-title="{channel["category"]}",{channel["name"]}\n')
                f.write(f'{channel["url"]}\n')
    total_working = len(unique_urls)
    print("\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ---")
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π –ø–ª–µ–π–ª–∏—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {OUTPUT_FILE}")
    print(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏ —Ä–∞–±–æ—á–∏—Ö –∫–∞–Ω–∞–ª–æ–≤: {total_working}")
    print(f"üóëÔ∏è  –û—Ç—Å–µ—è–Ω–æ –Ω–µ—Ä–∞–±–æ—á–∏—Ö –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(all_channels) - total_working}")

if __name__ == '__main__':
    asyncio.run(main())
